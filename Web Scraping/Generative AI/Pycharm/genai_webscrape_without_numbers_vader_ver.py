# -*- coding: utf-8 -*-
"""GenAI_webscrape_without_numbers_Vader_ver.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tEZTnYlg8cshlB8ha68c4gE6b_CI7MXm
"""

# Downloading libraries for web-scraping
!pip install requests
!pip install beautifulsoup4==4.9.3
!pip install bs4
!pip install html5lib
!pip install num2words
!pip install vaderSentiment

# Importing libraries for web-scraping and tokenization purposes
from bs4 import BeautifulSoup as bs
import requests
import nltk
import pandas as pd
import re
from textblob import TextBlob
from num2words import num2words
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# The list of common words such as "a", "an", etc.
nltk.download('stopwords')
# Download a collection of popular resources from the NLTK library
nltk.download('popular', quiet=True)
# Sentence tokenization (Splitting a text into individual senteces)
nltk.download('punkt')
# English vocabulary database
nltk.download('wordnet')
# Used for training language models or evaluating nlp algothms
nltk.download('brown')
# Used for performing sentiment analysis on text
nltk.download('vader_lexicon')

from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize

lemmatizer = WordNetLemmatizer()
analyzer = SentimentIntensityAnalyzer()
stemmer = SnowballStemmer("english")
stop_words = stopwords.words("english")

sentences = []

def scrape_clean (url, div_class):
  # Load the website
  website = requests.get(url).text
  soup = bs(website,'html.parser')

  # Find the div section that is the parent of all paragraphs
  div = soup.find_all('div', attrs={'class':div_class})

  # Get the text from paragraphs
  extracted_text = []
  for div in div:
      paragraphs = div.find_all('p')
      for paragraph in paragraphs:
          extracted_text.append(paragraph.get_text(strip=True))  # Remove leading/trailing whitespace

  # Splitting the text into sentences, remove the stopwords and punctuations, and save the cleaned version in a list
  sentences = []
  for text in extracted_text:
    sentence = sent_tokenize(text)
    for item in sentence:
      # Convert to lowercase
      text_lowercase = item.lower()
      # Remove punctuation
      text_without_punctuation = re.sub(r"[^\w\s]", "", text_lowercase)
      # Remove stopwords and stem words
      tokens = word_tokenize(text_without_punctuation)
      new_tokens = []
      for word in tokens:
        if word.isnumeric():
          word = num2words(word)
          new_tokens.append(lemmatizer.lemmatize(word))
          continue
        elif word not in stop_words:
          new_tokens.append(lemmatizer.lemmatize(word))
          continue
      # Join tokens back into a string
      cleaned_text = " ".join(new_tokens)
      sentences.append(cleaned_text)

  return sentences

website1 = 'https://www.forrester.com/blogs/focus-on-gen-ai-will-drive-core-modernization-and-saas-adoption-in-2024/?utm_source=google&utm_medium=cpc&utm_campaign=generative-ai/'
class1 = 'col-lg-10 order-lg-1'
text1 = scrape_clean(website1, class1)
sentences.extend(text1)
text1

website2 = 'https://www.forrester.com/blogs/genais-world-changing-power-is-putting-knowledge-to-work/?utm_source=google&utm_medium=cpc&utm_campaign=generative-ai'
class2 = 'col-lg-8 order-lg-2'
text2 = scrape_clean(website2, class2)
sentences.extend(text2)
text2

website3 = 'https://www.forrester.com/blogs/genai-will-shatter-and-reconstruct-every-company-and-industry/?utm_source=google&utm_medium=cpc&utm_campaign=generative-ai/'
class3 = 'post-content post-body__item'
text3 = scrape_clean(website3, class3)
sentences.extend(text3)
text3

website4 = 'https://news.sky.com/story/ai-risks-up-to-eight-million-uk-job-losses-with-low-skilled-worst-hit-report-warns-13102214'
class4 = 'sdc-site-layout__col sdc-site-layout__col1'
text4 = scrape_clean(website4, class4)
sentences.extend(text4)
text4

website5 = 'https://www.prnewswire.com/news-releases/50-billion-opportunity-emerges-for-insurers-worldwide-from-generative-ais-potential-to-boost-revenues-and-take-out-costs-302103876.html'
class5 = 'col-lg-10 col-lg-offset-1'
text5 = scrape_clean(website5, class5)
sentences.extend(text5)
text5

website6 = 'https://venturebeat.com/ai/the-risks-and-rewards-of-generative-ai-in-software-development/'
class6 = 'article-content'
text6 = scrape_clean(website6, class6)
sentences.extend(text6)
text6

website7 = 'https://thecorner.eu/news-spain/spain-economy/significant-growth-and-revenue-capture-opportunity-for-companies-serving-generative-ai-energy-needs-including-iberdrola/113769/'
class7 = 'entry-content clearfix'
text7 = scrape_clean(website7, class7)
sentences.extend(text7)
sentences

website8 = 'https://www.ox.ac.uk/news/2024-02-22-experts-call-responsible-use-generative-ai-adult-social-care'
class8 = 'field field-name-field-body field-type-text-with-summary field-label-hidden'
text8 = scrape_clean(website8, class8)
sentences.extend(text8)
sentences

website9 = 'https://theconversation.com/generative-ai-is-changing-the-legal-profession-future-lawyers-need-to-know-how-to-use-it-225730'
class9 = 'grid-ten large-grid-nine grid-last content-body content entry-content instapaper_body inline-promos'
text9 = scrape_clean(website9, class9)
sentences.extend(text9)
sentences

website10 = 'https://economymiddleeast.com/news/why-businesses-are-approaching-generative-ai-with-caution/'
class10 = 'brxe-post-content'
text10 = scrape_clean(website10, class10)
sentences.extend(text10)
sentences

website11 = 'https://www.globalvillagespace.com/tech/understanding-the-potential-benefits-and-dangers-of-generative-ai-in-software-development/'
class11 = 'tdb-block-inner td-fix-index'
text11 = scrape_clean(website11, class11)
sentences.extend(text11)
sentences

website12 = 'https://www.ox.ac.uk/news/2023-09-25-generative-ai-could-transform-work-boosting-productivity-and-democratising'
class12 = 'field field-name-field-body field-type-text-with-summary field-label-hidden'
text12 = scrape_clean(website12, class12)
sentences.extend(text12)
sentences

website13 = 'https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html'
class13 = 'ArticleBody-articleBody'
text13 = scrape_clean(website13, class13)
sentences.extend(text13)
sentences

website14 = 'https://www.unite.ai/beyond-generative-ai-building-a-comprehensive-and-scalable-digital-infrastructure/'
class14 = 'mvp-post-soc-out right relative'
text14 = scrape_clean(website14, class14)
sentences.extend(text14)
sentences

website15 = 'https://www.epam.com/about/newsroom/in-the-news/2024/for-creating-real-value-with-generative-ai-put-people-at-the-center'
class15 = 'text'
text15 = scrape_clean(website15, class15)
sentences.extend(text15)
sentences

website16 = 'https://www.reinsurancene.ws/50bn-opportunity-opportunity-arises-for-insurers-worldwide-from-genai-bain-company/'
class16 = 'pf-content'
text16 = scrape_clean(website16, class16)
sentences.extend(text16)
sentences

website17 = 'https://finance.yahoo.com/news/openai-makes-chatgpts-accessible-without-171737585.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAG5mu38jctSM9NyGRQWT0zmFdhgRGK_08M2n9GoZ3gG7IkHCE7-6tHVj5lUUdDzuVzKsj8GW1UJwaO9S4b99ompBG2BAZB4w9t83yyfhakzi9CW_TW4Oue0TDL95YO6IF0syszLJwwJnTkppnxu5JzZ6erc8_Npf4SFfhSVCRIVl'
class17 = 'caas-content-wrapper'
text17 = scrape_clean(website17, class17)
sentences.extend(text17)
sentences

website18 = 'https://www.euractiv.com/section/artificial-intelligence/infographic/generative-ai-puts-trust-in-the-news-media-to-the-test/'
class18 = 'ea-article-body-content'
text18 = scrape_clean(website18, class18)
sentences.extend(text18)
sentences

website19 = 'https://www.verdict.co.uk/generative-ai-unfounded-fears/'
class19 = 'cell large-8 main-content'
text19 = scrape_clean(website19, class19)
sentences.extend(text19)
sentences

website20 = 'https://itwire.com/business-it-news/enterprise-solutions/vonage-launches-generative-ai-for-conversational-commerce-solution.html'
class20 = 'itemFullText'
text20 = scrape_clean(website20, class20)
sentences.extend(text20)
sentences

for i in sentences:
  print(f'{i}\n')

# Use Vader library to get the polarity of the sentence
def getPolarity(text):
    polarity = SentimentIntensityAnalyzer().polarity_scores(text)
    if polarity['compound'] > 0:
      sentiment = 'postive'
    elif polarity['compound'] < 0:
      sentiment = 'negative'
    else:
      sentiment = 'neutral'
    return sentiment,polarity

for sentence in sentences:
  sentiment, polarity = getPolarity(sentence)
  print('\n', sentence)
  print(f"{sentiment}, {polarity}")

data = [] #List to store dictioneries
for sentence in sentences:
  sentiment, polarity = getPolarity(sentence)
  new_item = {'sentence' : sentence, 'sentiment' : sentiment, 'category' : 'GenAI'}
  data.append(new_item)

df = pd.DataFrame(data)

df.to_csv('GenAI (No numbers) - Vader ver.csv', index=False)