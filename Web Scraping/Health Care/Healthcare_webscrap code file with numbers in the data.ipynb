{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhcFh6hILqUL",
        "outputId": "467b85e0-d7ed-4242-e5f9-9178182e3d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Collecting beautifulsoup4==4.9.3\n",
            "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.8/115.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.9.3) (2.5)\n",
            "Installing collected packages: beautifulsoup4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.37 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.9.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beautifulsoup4-4.9.3\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.9.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib) (0.5.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Downloading libraries for web-scraping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4==4.9.3\n",
        "!pip install bs4\n",
        "!pip install html5lib\n",
        "\n",
        "# Importing libraries for web-scraping and tokenization purposes\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "# The list of common words such as \"a\", \"an\", etc.\n",
        "nltk.download('stopwords')\n",
        "# Download a collection of popular resources from the NLTK library\n",
        "nltk.download('popular', quiet=True)\n",
        "# Sentence tokenization (Splitting a text into individual senteces)\n",
        "nltk.download('punkt')\n",
        "# English vocabulary database\n",
        "nltk.download('wordnet')\n",
        "# Used for training language models or evaluating nlp algothms\n",
        "nltk.download('brown')\n",
        "# Used for performing sentiment analysis on text\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPsFPgS_3yA2"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "def scrape_clean (url, div_class,id_name=None):\n",
        "  # Load the website\n",
        "  website = requests.get(url).text\n",
        "  soup = bs(website,'html.parser')\n",
        "\n",
        "  # Find the div section that is the parent of all paragraphs\n",
        "  div = soup.find_all('div', attrs={'class':div_class, 'id':id_name})\n",
        "\n",
        "  # Get the text from paragraphs\n",
        "  extracted_text = []\n",
        "  for div in div:\n",
        "      paragraphs = div.find_all('p')\n",
        "      for paragraph in paragraphs:\n",
        "          extracted_text.append(paragraph.get_text(strip=True))  # Remove leading/trailing whitespace\n",
        "\n",
        "  # Splitting the text into sentences, remove the stopwords and punctuations, and save the cleaned version in a list\n",
        "  sentences = []\n",
        "  for text in extracted_text:\n",
        "    sentence = sent_tokenize(text)\n",
        "    for item in sentence:\n",
        "      # Convert to lowercase\n",
        "      text_lowercase = item.lower()\n",
        "      # Remove punctuation\n",
        "      text_without_punctuation = re.sub(r\"[^\\w\\s]\", \"\", text_lowercase)\n",
        "      # Remove stopwords and stem words\n",
        "      tokens = word_tokenize(text_without_punctuation)\n",
        "      tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "      # Join tokens back into a string\n",
        "      cleaned_text = \" \".join(tokens)\n",
        "      sentences.append(cleaned_text)\n",
        "\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4VrDBYR4kEU"
      },
      "outputs": [],
      "source": [
        "website1 = 'https://www.pewresearch.org/science/2023/02/22/60-of-americans-would-be-uncomfortable-with-provider-relying-on-ai-in-their-own-health-care/'\n",
        "class1 = 'post-content'\n",
        "id1 = None\n",
        "text1 = scrape_clean(website1, class1, id1)\n",
        "sentences.extend(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEEkCKyS4934"
      },
      "outputs": [],
      "source": [
        "website2 = 'https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-019-02204-y'\n",
        "class2 ='c-article-section__content'\n",
        "id2 = 'Sec13-content'\n",
        "text2 = scrape_clean(website2, class2, id2)\n",
        "sentences.extend(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZMSnfI30LtL"
      },
      "outputs": [],
      "source": [
        "website3 = 'https://foreseemed.com/artificial-intelligence-in-healthcare'\n",
        "class3 = [\"sqs-block html-block sqs-block-html\",\"sqs-block html-block sqs-block-html\"]\n",
        "id3 = [\"block-yui_3_17_2_1_1659634949407_20116\",\"block-yui_3_17_2_1_1677014147372_139146\"]\n",
        "text3 = scrape_clean(website3, class3,id3)\n",
        "sentences.extend(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtR4oBAqFmbu"
      },
      "outputs": [],
      "source": [
        "website4 = 'https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-023-04698-z#Sec7'\n",
        "class4 = 'c-article-section__content'\n",
        "id4 = \"Sec8-content\"\n",
        "text4 = scrape_clean(website4, class4,id4)\n",
        "sentences.extend(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC_ptYfmG1BA"
      },
      "outputs": [],
      "source": [
        "website5 = 'https://svn.bmj.com/content/2/4/230'\n",
        "class5 = 'section'\n",
        "id5 = \"sec-16\"\n",
        "text5 = scrape_clean(website5, class5, id5)\n",
        "sentences.extend(text5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B-h2ZieKE4s"
      },
      "outputs": [],
      "source": [
        "website6 = 'https://www.mckinsey.com/industries/healthcare/our-insights/transforming-healthcare-with-ai'\n",
        "class6 = 'mdc-o-content-body mck-u-dropcap'\n",
        "id6 = None\n",
        "text6 = scrape_clean(website6, class6, id6)\n",
        "sentences.extend(text6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW8CkgnNO5bL"
      },
      "outputs": [],
      "source": [
        "website7 = 'https://www.technologyreview.com/2020/01/22/276128/how-ai-is-humanizing-health-care/?utm_source=google&utm_medium=search&utm_campaign=acq_UKBLOW&utm_content=DSATOPICS&gad_source=1&gclid=Cj0KCQjw2a6wBhCVARIsABPeH1sNptGNGF9jblaq0_x0nJ3rvDIG9FuuDwKeI0Gnl2zBBurM94Yxpj4aAt5UEALw_wcB'\n",
        "class7 = 'gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2'\n",
        "id7 = None\n",
        "text7 = scrape_clean(website7, class7, id7)\n",
        "sentences.extend(text7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z11CBq_NSI-R"
      },
      "outputs": [],
      "source": [
        "website8 = 'https://www.healthcaredive.com/news/artificial-intelligence-AI-healthcare-patients-uncomfortable-Pew-research-center/643429/'\n",
        "class8 = \"medium-10 medium-centered large-12\"\n",
        "id8 = None\n",
        "text8 = scrape_clean(website8, class8, id8)\n",
        "sentences.extend(text8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCHjqKKHsybc"
      },
      "outputs": [],
      "source": [
        "website9 = 'https://www.healthcaredive.com/news/artificial-intelligence-AI-healthcare-patients-uncomfortable-Pew-research-center/643429/'\n",
        "class9 = \"medium-10 medium-centered large-12\"\n",
        "id9 = None\n",
        "text9 = scrape_clean(website9, class9, id9)\n",
        "sentences.extend(text9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "putzz0essyye"
      },
      "outputs": [],
      "source": [
        "website10 = 'https://www.medpagetoday.com/popmedicine/popmedicine/105330'\n",
        "class10 = \"main-content-region mpt-article-page\"\n",
        "id10 = \"js-main-content-region\"\n",
        "text10 = scrape_clean(website10, class10, id10)\n",
        "sentences.extend(text10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbPswEwqlIyi"
      },
      "outputs": [],
      "source": [
        "website11 = 'https://www.medicaleconomics.com/view/patients-don-t-understand-use-of-ai-in-health-care-and-many-don-t-trust-it'\n",
        "class11 = \"blockText_blockContent__TbCXh\"\n",
        "id11 = None\n",
        "text11 = scrape_clean(website11, class11, id11)\n",
        "sentences.extend(text11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3nX4sHiDOTg"
      },
      "outputs": [],
      "source": [
        "website12 = 'https://www.digitalhealth.net/2023/08/patient-data-more-than-half-of-uk-public-dont-trust-ai/'\n",
        "class12 = \"entry-content\"\n",
        "id12 = None\n",
        "text12 = scrape_clean(website12, class12, id12)\n",
        "sentences.extend(text12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtXFerrmDOmJ"
      },
      "outputs": [],
      "source": [
        "website13 = 'https://www.healthcareitnews.com/news/survey-1-3-patients-comfortable-ai-led-primary-care'\n",
        "class13 = \"field-item even\"\n",
        "id13 = None\n",
        "text13 = scrape_clean(website13, class13, id13)\n",
        "sentences.extend(text13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av3w5Tyga1UX"
      },
      "outputs": [],
      "source": [
        "website14 = 'https://www.prnewswire.com/news-releases/a-majority-of-americans-are-optimistic-that-ai-will-improve-healthcare-in-2024-301986068.html'\n",
        "class14 = \"col-lg-10 col-lg-offset-1\"\n",
        "id14 = None\n",
        "text14 = scrape_clean(website14, class14, id14)\n",
        "sentences.extend(text14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "670wZfpRdkWj"
      },
      "outputs": [],
      "source": [
        "website15 = 'https://www.mobihealthnews.com/news/virtual-second-opinions-are-popular-wariness-persists-ai-diagnosis-tools'\n",
        "class15 = \"region region-content\"\n",
        "id15 = None\n",
        "text15 = scrape_clean(website15, class15, id15)\n",
        "sentences.extend(text15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V22a16PmFssP"
      },
      "outputs": [],
      "source": [
        "website16 = 'https://mobius.md/2023/12/13/how-do-patients-feel-about-ai-in-healthcare/'\n",
        "class16 = \"entry-content\"\n",
        "id16 = None\n",
        "text16 = scrape_clean(website16, class16, id16)\n",
        "sentences.extend(text16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LwoCdg0O8n5"
      },
      "outputs": [],
      "source": [
        "website17 = 'https://www.urologytimes.com/view/survey-shows-patient-mistrust-of-ai-use-in-health-care'\n",
        "class17 = \"blockText_blockContent__TbCXh\"\n",
        "id17 = None\n",
        "text17 = scrape_clean(website17, class17, id17)\n",
        "sentences.extend(text17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef3nfwkvPRXh"
      },
      "outputs": [],
      "source": [
        "for i in sentences:\n",
        "  print(f'{i}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9uTk62Z8O6"
      },
      "outputs": [],
      "source": [
        "# Use textblob library to get the polarity of the sentence\n",
        "def getPolarity(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0:\n",
        "      sentiment = 'postive'\n",
        "    elif polarity < 0:\n",
        "      sentiment = 'negative'\n",
        "    else:\n",
        "      sentiment = 'neutral'\n",
        "    return sentiment,polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aquyu2HiGzot"
      },
      "outputs": [],
      "source": [
        "for sentence in sentences:\n",
        "   sentiment, polarity = getPolarity(sentence)\n",
        "   print('\\n', sentence)\n",
        "   print(f\"{sentiment}, {polarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzZfG2PbaSRN"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for sentence in sentences:\n",
        "  sentiment, polarity = getPolarity(sentence)\n",
        "  new_item = {'sentence' : sentence, 'sentiment' : sentiment, 'category': 'Health Care'}\n",
        "  data.append(new_item)\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsi702IVZDUZ"
      },
      "outputs": [],
      "source": [
        "df.to_csv('healthcare.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}