{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhcFh6hILqUL",
        "outputId": "8b4ade74-528f-4246-ed30-1eb104f226fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Collecting beautifulsoup4==4.9.3\n",
            "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.8/115.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.9.3) (2.5)\n",
            "Installing collected packages: beautifulsoup4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.37 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.9.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beautifulsoup4-4.9.3\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.9.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib) (0.5.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Downloading libraries for web-scraping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4==4.9.3\n",
        "!pip install bs4\n",
        "!pip install html5lib\n",
        "\n",
        "# Importing libraries for web-scraping and tokenization purposes\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "# The list of common words such as \"a\", \"an\", etc.\n",
        "nltk.download('stopwords')\n",
        "# Download a collection of popular resources from the NLTK library\n",
        "nltk.download('popular', quiet=True)\n",
        "# Sentence tokenization (Splitting a text into individual senteces)\n",
        "nltk.download('punkt')\n",
        "# English vocabulary database\n",
        "nltk.download('wordnet')\n",
        "# Used for training language models or evaluating nlp algothms\n",
        "nltk.download('brown')\n",
        "# Used for performing sentiment analysis on text\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPsFPgS_3yA2"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "def scrape_clean (url, div_class,id_name=None):\n",
        "  # Load the website\n",
        "  website = requests.get(url).text\n",
        "  soup = bs(website,'html.parser')\n",
        "\n",
        "  # Find the div section that is the parent of all paragraphs\n",
        "  div = soup.find_all('div', attrs={'class':div_class, 'id':id_name})\n",
        "\n",
        "  # Get the text from paragraphs\n",
        "  extracted_text = []\n",
        "  for div in div:\n",
        "      paragraphs = div.find_all('p')\n",
        "      for paragraph in paragraphs:\n",
        "          extracted_text.append(paragraph.get_text(strip=True))  # Remove leading/trailing whitespace\n",
        "\n",
        "  # Splitting the text into sentences, remove the stopwords and punctuations, and save the cleaned version in a list\n",
        "  sentences = []\n",
        "  for text in extracted_text:\n",
        "    sentence = sent_tokenize(text)\n",
        "    for item in sentence:\n",
        "      # Convert to lowercase\n",
        "      text_lowercase = item.lower()\n",
        "      # Remove punctuation\n",
        "      text_without_punctuation = re.sub(r\"[^\\w\\s]\", \"\", text_lowercase)\n",
        "      # Remove stopwords and stem words\n",
        "      tokens = word_tokenize(text_without_punctuation)\n",
        "      tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "      # Join tokens back into a string\n",
        "      cleaned_text = \" \".join(tokens)\n",
        "      sentences.append(cleaned_text)\n",
        "\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4VrDBYR4kEU"
      },
      "outputs": [],
      "source": [
        "website1 = 'https://educationhub.blog.gov.uk/2023/12/06/artificial-intelligence-in-schools-everything-you-need-to-know/'\n",
        "class1 = 'entry-content'\n",
        "text1 = scrape_clean(website1, class1)\n",
        "sentences.extend(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEEkCKyS4934"
      },
      "outputs": [],
      "source": [
        "website2 = 'https://www.linkedin.com/pulse/review-ai-education-huzaifa-khan-rktgf'\n",
        "class2 = 'counter-list'\n",
        "text2 = scrape_clean(website2, class2)\n",
        "sentences.extend(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TQkktTLQ-ZF"
      },
      "outputs": [],
      "source": [
        "website3 = 'https://slidesgo.com/slidesgo-school/news/ai-in-education-survey-exclusive-slidesgo-insights-of-ai-tools-for-education'\n",
        "class3 = 'counter-list'\n",
        "id3 = None\n",
        "text3 = scrape_clean(website3, class3, id3)\n",
        "sentences.extend(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcGcM7_vXONP"
      },
      "outputs": [],
      "source": [
        "website4 = 'https://www.insidehighered.com/news/student-success/life-after-college/2024/01/10/survey-college-students-thoughts-ai-and-careers'\n",
        "class4 = 'node-content'\n",
        "id4 = None\n",
        "text4 = scrape_clean(website4, class4, id4)\n",
        "sentences.extend(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvnXpBxJYpnP"
      },
      "outputs": [],
      "source": [
        "website5 = 'https://today.yougov.com/technology/articles/45607-most-think-schools-should-teach-about-ai-poll'\n",
        "class5 = 'app-container ng-tns-c2792280337-0 ng-trigger ng-trigger-routeAnimation'\n",
        "id5 = None\n",
        "text5 = scrape_clean(website5, class5, id5)\n",
        "sentences.extend(text5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHA6tsfGcCf-"
      },
      "outputs": [],
      "source": [
        "website6 = 'https://www.the74million.org/article/how-do-teachers-feel-about-their-jobs-the-impact-of-ai-new-survey-has-answers/'\n",
        "class6 = 'article_content'\n",
        "id6 = None\n",
        "text6 = scrape_clean(website6, class6, id6)\n",
        "sentences.extend(text6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5MEXI7fc8kW"
      },
      "outputs": [],
      "source": [
        "website7 = 'https://www.theguardian.com/technology/2024/feb/01/more-than-half-uk-undergraduates-ai-essays-artificial-intelligence'\n",
        "class7 = 'dcr-lw02qf'\n",
        "id7 = \"maincontent\"\n",
        "text7 = scrape_clean(website7, class7, id7)\n",
        "sentences.extend(text7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC8O9n2tfNux"
      },
      "outputs": [],
      "source": [
        "website8 = 'https://www.forbes.com/sites/nickmorrison/2023/05/31/half-of-teachers-believe-ai-will-change-education-for-the-better/'\n",
        "class8 = 'article-body fs-article fs-responsive-text current-article'\n",
        "id8 = None\n",
        "text8 = scrape_clean(website8, class8, id8)\n",
        "sentences.extend(text8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzUL--ZngvW7"
      },
      "outputs": [],
      "source": [
        "website9 = 'https://www.learner.com/blog/ai-education-survey'\n",
        "class9 = 'w-node-_6b677d7e-03e7-913b-97b5-19c037c52658-3294f5cd'\n",
        "id9 = 'content'\n",
        "text9 = scrape_clean(website9, class9, id9)\n",
        "sentences.extend(text9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFKj176R551e"
      },
      "outputs": [],
      "source": [
        "website10 = 'https://doodlelearning.com/us/math/guides/ai-in-education'\n",
        "class10 = 'elementor-widget-container'\n",
        "id10 = None\n",
        "text10 = scrape_clean(website10, class10, id10)\n",
        "sentences.extend(text10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD92JpCD8hjf"
      },
      "outputs": [],
      "source": [
        "website11 = 'https://www.the74million.org/article/national-chatgpt-survey-teachers-accepting-ai-into-classrooms-workflow-even-more-than-'\n",
        "class11 = None\n",
        "id11 = 'content'\n",
        "text11 = scrape_clean(website11, class11, id11)\n",
        "sentences.extend(text11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trt1YWES90vj"
      },
      "outputs": [],
      "source": [
        "website12 = 'https://thetutorsassociation.org.uk/2023/09/27/students-begin-to-embrace-ai-for-learning-survey-shows/'\n",
        "class12 = 'entry-content clear'\n",
        "id12 = None\n",
        "text12 = scrape_clean(website12, class12, id12)\n",
        "sentences.extend(text12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IbQrkHbXAdyo"
      },
      "outputs": [],
      "source": [
        "website13 = 'https://thetutorsassociation.org.uk/2023/09/27/students-begin-to-embrace-ai-for-learning-survey-shows/'\n",
        "class13 = 'entry-content clear'\n",
        "id13 = None\n",
        "text13 = scrape_clean(website13, class13, id13)\n",
        "sentences.extend(text13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EphvCcJkBt_F"
      },
      "outputs": [],
      "source": [
        "website14 = 'https://www.fenews.co.uk/skills/new-research-ai-for-education-in-the-uk-does-the-potential-outweigh-the-risk/'\n",
        "class14 = 'entry-content'\n",
        "id14 = None\n",
        "text14 = scrape_clean(website14, class14, id14)\n",
        "sentences.extend(text14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7k8CH11C0hJ"
      },
      "outputs": [],
      "source": [
        "website15 = 'https://www.theguardian.com/commentisfree/2023/jul/14/ai-artificial-intelligence-disrupt-education-creativity-critical-thinking'\n",
        "class15 = 'dcr-lw02qf'\n",
        "id15 = 'maincontent'\n",
        "text15 = scrape_clean(website15, class15, id15)\n",
        "sentences.extend(text15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef3nfwkvPRXh"
      },
      "outputs": [],
      "source": [
        "for i in sentences:\n",
        "  print(f'{i}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9uTk62Z8O6"
      },
      "outputs": [],
      "source": [
        "# Use textblob library to get the polarity of the sentence\n",
        "def getPolarity(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0:\n",
        "      sentiment = 'postive'\n",
        "    elif polarity < 0:\n",
        "      sentiment = 'negative'\n",
        "    else:\n",
        "      sentiment = 'neutral'\n",
        "    return sentiment,polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzZfG2PbaSRN"
      },
      "outputs": [],
      "source": [
        "for sentence in sentences:\n",
        "  sentiment, polarity = getPolarity(sentence)\n",
        "  print('\\n', sentence)\n",
        "  print(f\"{sentiment}, {polarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaOrH6wByt27"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for sentence in sentences:\n",
        "  sentiment, polarity = getPolarity(sentence)\n",
        "  new_item = {'sentence' : sentence, 'sentiment' : sentiment, 'category': 'Education'}\n",
        "  data.append(new_item)\n",
        "\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rymHdeAmyxGk"
      },
      "outputs": [],
      "source": [
        "df.to_csv('education.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}