# -*- coding: utf-8 -*-
"""FashionAI_webscrape_without_numbers_Vader_ver.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFxcpPBTR5CAcFQRJexlSgx9xiqCd-Zj
"""

# Downloading libraries for web-scraping
!pip install requests
!pip install beautifulsoup4==4.9.3
!pip install bs4
!pip install html5lib
!pip install num2words
!pip install vaderSentiment

# Importing libraries for web-scraping and tokenization purposes
from bs4 import BeautifulSoup as bs
import requests
import nltk
import pandas as pd
import re
from textblob import TextBlob
from num2words import num2words
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# The list of common words such as "a", "an", etc.
nltk.download('stopwords')
# Download a collection of popular resources from the NLTK library
nltk.download('popular', quiet=True)
# Sentence tokenization (Splitting a text into individual senteces)
nltk.download('punkt')
# English vocabulary database
nltk.download('wordnet')
# Used for training language models or evaluating nlp algothms
nltk.download('brown')
# Used for performing sentiment analysis on text
nltk.download('vader_lexicon')

from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize

lemmatizer = WordNetLemmatizer()
analyzer = SentimentIntensityAnalyzer()
stemmer = SnowballStemmer("english")
stop_words = stopwords.words("english")

sentences = []

def scrape_clean (url, div_class):
  # Load the website
  website = requests.get(url).text
  soup = bs(website,'html.parser')

  # Find the div section that is the parent of all paragraphs
  div = soup.find_all('div', attrs={'class':div_class})

  # Get the text from paragraphs
  extracted_text = []
  for div in div:
      paragraphs = div.find_all('p')
      for paragraph in paragraphs:
          extracted_text.append(paragraph.get_text(strip=True))  # Remove leading/trailing whitespace

  # Splitting the text into sentences, remove the stopwords and punctuations, and save the cleaned version in a list
  sentences = []
  for text in extracted_text:
    sentence = sent_tokenize(text)
    for item in sentence:
      # Convert to lowercase
      text_lowercase = item.lower()
      # Remove punctuation
      text_without_punctuation = re.sub(r"[^\w\s]", "", text_lowercase)
      # Remove stopwords and stem words
      tokens = word_tokenize(text_without_punctuation)
      new_tokens = []
      for word in tokens:
        if word.isnumeric():
          word = num2words(word)
          new_tokens.append(lemmatizer.lemmatize(word))
          continue
        elif word not in stop_words:
          new_tokens.append(lemmatizer.lemmatize(word))
          continue
      # Join tokens back into a string
      cleaned_text = " ".join(new_tokens)
      sentences.append(cleaned_text)

  return sentences

website1 = 'https://www.theguardian.com/fashion/2024/feb/08/ai-london-fashion-week'
class1 = 'dcr-ch7w1w'
text1 = scrape_clean(website1, class1)
sentences.extend(text1)
sentences

website2 = 'https://www.bbc.co.uk/news/business-68347250'
class2 = 'ssrcss-1ki8hfp-StyledZone e1mcntqj3'
text2 = scrape_clean(website2, class2)
sentences.extend(text2)
sentences

website3 = 'https://ytech.news/en/the-integral-role-of-ai-in-the-future-of-fashion/'
class3 = 'post-content-inner'
text3 = scrape_clean(website3, class3)
sentences.extend(text3)
sentences

website4 = 'https://www.aljazeera.com/economy/2024/2/23/the-latest-industry-upset-with-the-use-of-ai-fashion'
class4 = 'wysiwyg wysiwyg--all-content css-ibbk12'
text4 = scrape_clean(website4, class4)
sentences.extend(text4)
sentences

website5 = 'https://www.just-style.com/news/signal-upskilling-in-ai-could-be-the-next-big-requirement-in-fashion/?cf-view'
class5 = 'cell large-8 main-content'
text5 = scrape_clean(website5, class5)
sentences.extend(text5)
sentences

website6 = 'https://www.forbes.com/sites/forbescommunicationscouncil/2023/11/30/the-future-of-marketing-in-the-fashion-and-lifestyle-industries-ai-personalization-and-data-driven-insights/?sh=2eeb47975f0e'
class6 = 'body-container'
text6 = scrape_clean(website6, class6)
sentences.extend(text6)
sentences

website7 = 'https://fashionunited.uk/news/business/gen-ai-game-changer-for-sustainable-fashion/2024010373369'
class7 = 'css-1s0my6s e15wwp330'
text7 = scrape_clean(website7, class7)
sentences.extend(text7)
sentences

website8 = 'https://www.standard.co.uk/news/tech/ai-fashion-models-virtual-tryon-diffusion-models-b1120382.html'
class8 = 'sc-ghHZHN kNjFqH'
text8 = scrape_clean(website8, class8)
sentences.extend(text8)
sentences

website9 = 'https://jingdaily.com/posts/web3-ai-and-the-fashion-revolution-insights-from-the-2023-new-codes-digital-fashion-summit'
class9 = 'c-cvYqge'
text9 = scrape_clean(website9, class9)
sentences.extend(text9)
sentences

website10 = 'https://www.context.news/ai/digital-designs-to-avatar-models-how-is-ai-transforming-fashion'
class10 = 'ArticleText_text__9RYj1'
text10 = scrape_clean(website10, class10)
sentences.extend(text10)
sentences

website11 = 'https://www.glossy.co/fashion/how-generative-ai-will-impact-fashion/'
class11 = 'article-content'
text11 = scrape_clean(website11, class11)
sentences.extend(text11)
sentences

website12 = 'https://www.just-style.com/news/new-study-suggests-ai-will-revolutionise-fashion-design/'
class12 = 'cell large-8 main-content'
text12 = scrape_clean(website12, class12)
sentences.extend(text12)
sentences

website13 = 'https://www.forbes.com/sites/cognitiveworld/2019/07/16/the-fashion-industry-is-getting-more-intelligent-with-ai/?sh=496600243c74'
class13 = 'article-body fs-article fs-responsive-text current-article'
text13 = scrape_clean(website13, class13)
sentences.extend(text13)
sentences

website14 = 'https://fashionunited.uk/news/fashion/ai-in-fashion-interview-with-vp-of-inspire-and-engage-at-zalando/2019080944674'
class14 = 'css-1s0my6s e15wwp330'
text14 = scrape_clean(website14, class14)
sentences.extend(text14)
sentences

website15 = 'https://metro.co.uk/2020/03/04/will-wearing-clothes-designed-artificial-intelligence-12342122/'
class15 = 'article-body'
text15 = scrape_clean(website15, class15)
sentences.extend(text15)
sentences

website16 = 'https://fashionunited.uk/news/fashion/fashion-in-the-age-of-ai-from-design-to-customer-experience/2023031768533'
class16 = 'css-1s0my6s e15wwp330'
text16 = scrape_clean(website16, class16)
sentences.extend(text16)
sentences

website17 = 'https://www.nbcnews.com/business/business-news/ai-models-levis-controversy-backlash-rcna77280'
class17 = 'article-body__content'
text17 = scrape_clean(website17, class17)
sentences.extend(text17)
sentences

website18 = 'https://www.standard.co.uk/business/ai-uk-fashion-retail-robots-b1086114.html'
class18 = 'sc-ghHZHN kNjFqH'
text18 = scrape_clean(website18, class18)
sentences.extend(text18)
sentences

website19 = 'https://fashionunited.uk/news/fashion/how-ai-is-transforming-the-fashion-industry/2019081344722'
class19 = 'css-1s0my6s e15wwp330'
text19 = scrape_clean(website19, class19)
sentences.extend(text19)
sentences

website20 = 'https://www.just-style.com/news/signal-generative-ai-worth-risk-to-amplify-fashions-creative-process/?cf-view'
class20 = 'cell large-8 main-content'
text20 = scrape_clean(website20, class20)
sentences.extend(text20)
sentences

for i in sentences:
  print(f'{i}\n')

# Use Vader library to get the polarity of the sentence
def getPolarity(text):
    polarity = SentimentIntensityAnalyzer().polarity_scores(text)
    if polarity['compound'] > 0:
      sentiment = 'postive'
    elif polarity['compound'] < 0:
      sentiment = 'negative'
    else:
      sentiment = 'neutral'
    return sentiment,polarity

for sentence in sentences:
  sentiment, polarity = getPolarity(sentence)
  print('\n', sentence)
  print(f"{sentiment}, {polarity}")

data = [] #List to store dictioneries
for sentence in sentences:
  sentiment, polarity = getPolarity(sentence)
  new_item = {'sentence' : sentence, 'sentiment' : sentiment, 'category' : 'Fashion'}
  data.append(new_item)

df = pd.DataFrame(data)

df.to_csv('Fashion (no numbers) - Vader ver.csv', index=False)